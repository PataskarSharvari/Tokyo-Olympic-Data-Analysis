{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4593ff",
   "metadata": {},
   "source": [
    "# üöÄ ETL Pipeline on Azure with PySpark (CSV Only)\n",
    "---\n",
    "This notebook demonstrates an **ETL pipeline** on **Azure Data Lake Storage Gen2 (ADLS Gen2)** using **PySpark**. Unlike the Parquet-based pipeline, here we focus on **CSV ingestion, transformation, and saving back as CSV**.\n",
    "\n",
    "### üîë Key Steps\n",
    "1. Configure ADLS access (OAuth-based)\n",
    "2. Extract raw CSV data (5 Olympic datasets)\n",
    "3. Perform data exploration & transformations\n",
    "4. Save transformed data back to ADLS in CSV format\n",
    "5. Verify outputs\n",
    "\n",
    "‚ö†Ô∏è **Note:** For GitHub upload, client ID/secret should be stored securely using Azure Key Vault or Databricks secrets, not hardcoded as shown here for demo purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cb502",
   "metadata": {},
   "source": [
    "## üì¶ Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f164a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be08d1e",
   "metadata": {},
   "source": [
    "## üîë Configure Azure Data Lake Access\n",
    "We configure Spark to authenticate with Azure Data Lake Storage Gen2 using **OAuth credentials**.\n",
    "\n",
    "üëâ Replace placeholders with your own credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fceb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.<storage_account>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<storage_account>.dfs.core.windows.net\",\n",
    "              \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<storage_account>.dfs.core.windows.net\", \"<client-id>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage_account>.dfs.core.windows.net\", \"<client-secret>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage_account>.dfs.core.windows.net\",\n",
    "              \"https://login.microsoftonline.com/<tenant-id>/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38833ba6",
   "metadata": {},
   "source": [
    "## üì• Step 1: Extract Raw CSV Data\n",
    "We read 5 Olympic datasets (Athletes, Coaches, EntriesGender, Medals, Teams) from the **raw-data** zone in ADLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = spark.read.csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/raw-data/Athletes.csv\",\n",
    "                          header=True, inferSchema=True)\n",
    "coaches = spark.read.csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/raw-data/Coaches.csv\",\n",
    "                         header=True, inferSchema=True)\n",
    "entries_gender = spark.read.csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/raw-data/EntriesGender.csv\",\n",
    "                                header=True, inferSchema=True)\n",
    "medals = spark.read.csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/raw-data/Medals.csv\",\n",
    "                        header=True, inferSchema=True)\n",
    "teams = spark.read.csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/raw-data/Teams.csv\",\n",
    "                       header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea4e6f",
   "metadata": {},
   "source": [
    "## üîç Step 2: Data Exploration\n",
    "Quick look at row counts and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbda8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Athletes rows:\", athletes.count())\n",
    "print(\"Coaches rows:\", coaches.count())\n",
    "print(\"EntriesGender rows:\", entries_gender.count())\n",
    "print(\"Medals rows:\", medals.count())\n",
    "print(\"Teams rows:\", teams.count())\n",
    "\n",
    "athletes.show(5)\n",
    "medals.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba978b",
   "metadata": {},
   "source": [
    "## üîÑ Step 3: Transformations\n",
    "- Cast gender counts to integers\n",
    "- Find top countries by gold medals\n",
    "- Calculate average female/male participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast numeric columns\n",
    "entries_gender = entries_gender.withColumn(\"Female\", col(\"Female\").cast(IntegerType())) \\\n",
    "                               .withColumn(\"Male\", col(\"Male\").cast(IntegerType())) \\\n",
    "                               .withColumn(\"Total\", col(\"Total\").cast(IntegerType()))\n",
    "\n",
    "# Top countries by gold medals\n",
    "top_gold_medal_countries = medals.orderBy(\"Gold\", ascending=False) \\\n",
    "                                 .select(\"TeamCountry\", \"Gold\")\n",
    "top_gold_medal_countries.show(10)\n",
    "\n",
    "# Average gender participation\n",
    "average_entries_by_gender = entries_gender.withColumn(\"Avg_Female\", col(\"Female\") / col(\"Total\")) \\\n",
    "                                               .withColumn(\"Avg_Male\", col(\"Male\") / col(\"Total\"))\n",
    "average_entries_by_gender.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433dbec",
   "metadata": {},
   "source": [
    "## üíæ Step 4: Save Transformed Data (CSV)\n",
    "We save the transformed datasets into the **transformed-data** zone.\n",
    "\n",
    "‚ö†Ô∏è Using `.repartition(1)` ensures single CSV output (useful for demos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd8698",
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes.repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/transformed-data/Athletes\")\n",
    "coaches.repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/transformed-data/Coaches\")\n",
    "entries_gender.repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/transformed-data/EntriesGender\")\n",
    "medals.repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/transformed-data/Medals\")\n",
    "teams.repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/transformed-data/Teams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab084ab",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 5: Verification\n",
    "Reload one dataset from the transformed zone to confirm schema and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d67268",
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes_check = spark.read.csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/transformed-data/Athletes\",\n",
    "                               header=True, inferSchema=True)\n",
    "\n",
    "athletes_check.show(5)\n",
    "athletes_check.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98283c",
   "metadata": {},
   "source": [
    "## üìå Summary\n",
    "In this notebook we:\n",
    "- Configured ADLS access via OAuth\n",
    "- Extracted raw CSV datasets\n",
    "- Performed basic transformations & analytics\n",
    "- Saved outputs back to ADLS in CSV format\n",
    "\n",
    "üëâ Next Steps:\n",
    "- Automate pipeline with **Azure Data Factory**\n",
    "- Add **CI/CD integration** for production\n",
    "- Build **Power BI dashboards** for reporting"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}