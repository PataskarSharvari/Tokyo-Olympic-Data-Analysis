{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd59770",
   "metadata": {},
   "source": [
    "\n",
    "# ETL Data Pipeline on Azure with PySpark (Tokyo Olympics)\n",
    "\n",
    "**Author:** Sharvari Pataskar  \n",
    "**Last updated:** 2025-08-19 17:50:24\n",
    "\n",
    "This notebook demonstrates an end-to-end **ETL (Extract–Transform–Load)** flow on **Azure Data Lake Storage Gen2 (ADLS Gen2)** using **PySpark**.\n",
    "\n",
    "**Datasets (CSV in ADLS raw zone):**\n",
    "- `Athletes.csv`\n",
    "- `Coaches.csv`\n",
    "- `EntriesGender.csv`\n",
    "- `Medals.csv`\n",
    "- `Teams.csv`\n",
    "\n",
    "**What this notebook covers**\n",
    "1. Environment & dependency setup  \n",
    "2. Secure Spark configuration for ADLS Gen2 (OAuth 2.0 Client Credentials)  \n",
    "3. **Extract** raw CSVs from ADLS  \n",
    "4. **Load** to a **curated zone** as **Parquet** (optimized columnar format)  \n",
    "5. **Transform** selected tables and derive **analytics**  \n",
    "6. **Load** transformed outputs to a dedicated **transformed zone**  \n",
    "7. **Verify** schemas and records, and discuss **next steps** for BI/Synapse\n",
    "\n",
    "> ⚠️ **Security Note:** Do **NOT** hardcode secrets or keys in source control. In this notebook we use **placeholders**. Prefer using Databricks **Secrets**, Azure **Service Principals**, or MSI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c8216",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Prerequisites\n",
    "\n",
    "- **Azure resources**: ADLS Gen2 storage account + container with the Tokyo Olympics CSVs under `raw-data/`  \n",
    "- **Identity**: Azure AD **App registration** (Client ID, Client Secret, Tenant ID) or Managed Identity  \n",
    "- **Compute**: Databricks cluster (recommended) or local Spark with Hadoop Azure connectors  \n",
    "- **Permissions**: The SPN / identity must have **Storage Blob Data Contributor** on the storage account\n",
    "\n",
    "> Tip (Databricks): Store secrets in a **Secret Scope**, then read with `dbutils.secrets.get()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Databricks, you can %pip install packages per cluster or use a cluster init script.\n",
    "# In plain Jupyter, uncomment the line below.\n",
    "# !pip install azure-identity azure-storage-blob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96b50c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Configure Spark to access ADLS Gen2 (OAuth 2.0)\n",
    "\n",
    "Replace the placeholders below.  \n",
    "**Never commit real secrets** to GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== EDIT THESE VALUES (do NOT commit real secrets) ====\n",
    "STORAGE_ACCOUNT = \"tokyoolympicdatasharvari\"  # without suffix\n",
    "CONTAINER       = \"tokyoolympicdatasharvari\"  # container name\n",
    "TENANT_ID       = \"<TENANT_ID>\"\n",
    "CLIENT_ID       = \"<CLIENT_ID>\"\n",
    "CLIENT_SECRET   = \"<CLIENT_SECRET>\"  # Prefer getting from secret store e.g., dbutils.secrets.get()\n",
    "\n",
    "# Build DFS endpoint once (ADLS Gen2)\n",
    "DFS_HOST = f\"{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
    "\n",
    "# Optional: If using Databricks secrets, do something like:\n",
    "# CLIENT_SECRET = dbutils.secrets.get(scope=\"<scope-name>\", key=\"<key-name>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spark configs for OAuth2 Client Credentials flow\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{DFS_HOST}\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{DFS_HOST}\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{DFS_HOST}\", CLIENT_ID)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{DFS_HOST}\", CLIENT_SECRET)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{DFS_HOST}\", f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/token\")\n",
    "\n",
    "print(\"Spark configured for ADLS Gen2 (OAuth 2.0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3a1f5",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Define Storage Paths\n",
    "\n",
    "We organize data in zones: **raw-data** → **curated** → **transformed-data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_base        = f\"abfss://{CONTAINER}@{DFS_HOST}/raw-data\"\n",
    "curated_base    = f\"abfss://{CONTAINER}@{DFS_HOST}/curated\"\n",
    "transformed_base= f\"abfss://{CONTAINER}@{DFS_HOST}/transformed-data\"\n",
    "\n",
    "paths = {\n",
    "    \"athletes_csv\":        f\"{raw_base}/Athletes.csv\",\n",
    "    \"coaches_csv\":         f\"{raw_base}/Coaches.csv\",\n",
    "    \"entries_gender_csv\":  f\"{raw_base}/EntriesGender.csv\",\n",
    "    \"medals_csv\":          f\"{raw_base}/Medals.csv\",\n",
    "    \"teams_csv\":           f\"{raw_base}/Teams.csv\",\n",
    "    \"athletes_parquet\":    f\"{curated_base}/Athletes\",\n",
    "    \"coaches_parquet\":     f\"{curated_base}/Coaches\",\n",
    "    \"entries_parquet\":     f\"{curated_base}/EntriesGender\",\n",
    "    \"medals_parquet\":      f\"{curated_base}/Medals\",\n",
    "    \"teams_parquet\":       f\"{curated_base}/Teams\",\n",
    "    \"athletes_tx\":         f\"{transformed_base}/Athletes\",\n",
    "    \"coaches_tx\":          f\"{transformed_base}/Coaches\",\n",
    "    \"entries_tx\":          f\"{transformed_base}/EntriesGender\",\n",
    "    \"medals_tx\":           f\"{transformed_base}/Medals\",\n",
    "    \"teams_tx\":            f\"{transformed_base}/Teams\",\n",
    "}\n",
    "paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f807a5f",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Extract — Read raw CSVs from ADLS\n",
    "\n",
    "Set `header=True` for header rows and `inferSchema=True` for automatic type inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "athletes = (spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(paths[\"athletes_csv\"]))\n",
    "coaches = (spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(paths[\"coaches_csv\"]))\n",
    "entries_gender = (spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(paths[\"entries_gender_csv\"]))\n",
    "medals = (spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(paths[\"medals_csv\"]))\n",
    "teams = (spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(paths[\"teams_csv\"]))\n",
    "\n",
    "print(\"Sample preview of Athletes:\")\n",
    "athletes.show(5)\n",
    "athletes.printSchema()\n",
    "\n",
    "print(\"Row counts (raw):\")\n",
    "for name, df in [(\"Athletes\", athletes), (\"Coaches\", coaches), (\"EntriesGender\", entries_gender), (\"Medals\", medals), (\"Teams\", teams)]:\n",
    "    print(name, df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38d812",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Load — Write curated Parquet datasets\n",
    "\n",
    "Parquet is a compressed, columnar format that accelerates analytics workloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(athletes.write.mode(\"overwrite\").parquet(paths[\"athletes_parquet\"]))\n",
    "(coaches.write.mode(\"overwrite\").parquet(paths[\"coaches_parquet\"]))\n",
    "(entries_gender.write.mode(\"overwrite\").parquet(paths[\"entries_parquet\"]))\n",
    "(medals.write.mode(\"overwrite\").parquet(paths[\"medals_parquet\"]))\n",
    "(teams.write.mode(\"overwrite\").parquet(paths[\"teams_parquet\"]))\n",
    "\n",
    "print(\"Curated Parquet writes completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e748f5",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Quick Verification — Read curated Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db286e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "athletes_pq = spark.read.parquet(paths[\"athletes_parquet\"])\n",
    "coaches_pq = spark.read.parquet(paths[\"coaches_parquet\"])\n",
    "entries_pq = spark.read.parquet(paths[\"entries_parquet\"])\n",
    "medals_pq = spark.read.parquet(paths[\"medals_parquet\"])\n",
    "teams_pq = spark.read.parquet(paths[\"teams_parquet\"])\n",
    "\n",
    "for name, df in [(\"Athletes\", athletes_pq), (\"Coaches\", coaches_pq), (\"EntriesGender\", entries_pq), (\"Medals\", medals_pq), (\"Teams\", teams_pq)]:\n",
    "    print(f\"\\n{name} (curated) schema:\")\n",
    "    df.printSchema()\n",
    "    print(f\"{name} (curated) sample:\")\n",
    "    df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd2ff4",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Transform — Type casting and derived metrics\n",
    "\n",
    "We cast **EntriesGender** numeric columns and compute gender share.  \n",
    "We also rank countries by **Gold** medals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, round as _round\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Ensure numeric types\n",
    "entries_tx = (entries_pq\n",
    "              .withColumn(\"Female\", col(\"Female\").cast(IntegerType()))\n",
    "              .withColumn(\"Male\",   col(\"Male\").cast(IntegerType()))\n",
    "              .withColumn(\"Total\",  col(\"Total\").cast(IntegerType()))\n",
    "              .withColumn(\"Share_Female\", _round(col(\"Female\")/col(\"Total\"), 4))\n",
    "              .withColumn(\"Share_Male\",   _round(col(\"Male\")/col(\"Total\"), 4))\n",
    "             )\n",
    "\n",
    "print(\"Transformed EntriesGender schema:\")\n",
    "entries_tx.printSchema()\n",
    "entries_tx.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top countries by Gold medals (descending)\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Some datasets use 'Team/NOC' or 'TeamCountry'. Adjust column name if needed.\n",
    "gold_col = \"Gold\" if \"Gold\" in medals_pq.columns else None\n",
    "team_col = None\n",
    "for c in [\"TeamCountry\", \"Team/NOC\", \"Team\", \"Country\"]:\n",
    "    if c in medals_pq.columns:\n",
    "        team_col = c\n",
    "        break\n",
    "\n",
    "if gold_col and team_col:\n",
    "    top_gold = medals_pq.select(team_col, gold_col).orderBy(desc(gold_col))\n",
    "    print(\"Top 10 by Gold medals:\")\n",
    "    top_gold.show(10)\n",
    "else:\n",
    "    print(\"Expected columns for medals not found. Available columns:\", medals_pq.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c7368",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Load — Write transformed outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc80938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "athletes_tx = athletes_pq  # (no-op example; add business rules as needed)\n",
    "coaches_tx  = coaches_pq\n",
    "medals_tx   = medals_pq\n",
    "teams_tx    = teams_pq\n",
    "\n",
    "athletes_tx.write.mode(\"overwrite\").parquet(paths[\"athletes_tx\"])\n",
    "coaches_tx.write.mode(\"overwrite\").parquet(paths[\"coaches_tx\"])\n",
    "entries_tx.write.mode(\"overwrite\").parquet(paths[\"entries_tx\"])\n",
    "medals_tx.write.mode(\"overwrite\").parquet(paths[\"medals_tx\"])\n",
    "teams_tx.write.mode(\"overwrite\").parquet(paths[\"teams_tx\"])\n",
    "\n",
    "print(\"Transformed Parquet writes completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b70a3",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Validate Transformed Zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "athletes_check = spark.read.parquet(paths[\"athletes_tx\"])\n",
    "coaches_check = spark.read.parquet(paths[\"coaches_tx\"])\n",
    "entries_check = spark.read.parquet(paths[\"entries_tx\"])\n",
    "medals_check = spark.read.parquet(paths[\"medals_tx\"])\n",
    "teams_check = spark.read.parquet(paths[\"teams_tx\"])\n",
    "\n",
    "for name, df in [(\"Athletes_tx\", athletes_check), (\"Coaches_tx\", coaches_check), (\"EntriesGender_tx\", entries_check), (\"Medals_tx\", medals_check), (\"Teams_tx\", teams_check)]:\n",
    "    print(f\"\\n{name} schema:\")\n",
    "    df.printSchema()\n",
    "    print(f\"{name} sample:\")\n",
    "    df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bcef9b",
   "metadata": {},
   "source": [
    "\n",
    "## 10) (Optional) Useful Patterns & Hardening\n",
    "\n",
    "- **Partitioning**: When data is large, write Parquet partitioned by columns (e.g., `year`, `discipline`).  \n",
    "  ```python\n",
    "  df.write.mode(\"overwrite\").partitionBy(\"Discipline\").parquet(\".../path\")\n",
    "  ```\n",
    "\n",
    "- **Schema-on-read**: For production, define explicit schemas to avoid inference drift.  \n",
    "- **Delta Lake**: Switch to Delta for ACID upserts & time travel.  \n",
    "- **Secrets**: Use `dbutils.secrets.get(scope, key)` or Azure Key Vault.  \n",
    "- **Monitoring**: Track job runs via **Databricks Jobs** or **ADF**.  \n",
    "- **Downstream**: Connect **Power BI** or **Synapse Serverless** to curated/transformed zones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2edd59",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Conclusion & Next Steps\n",
    "\n",
    "✅ Built a clean ETL pipeline using PySpark on ADLS Gen2: **Extract → Curate (Parquet) → Transform → Validate**.  \n",
    "➡️ Extend with **Delta Lake**, add **data quality checks** (Great Expectations), and publish to a BI layer.\n",
    "\n",
    "**Repository Tips (for GitHub):**\n",
    "- Keep this notebook under `notebooks/etl_pipeline_tokyo_olympics.ipynb`\n",
    "- Add `docs/architecture.png` (simple diagram of Raw → Curated → Transformed)\n",
    "- Include a `README.md` describing setup, flow, and sample outputs\n",
    "- Add `.gitignore` to exclude checkpoints and local artifacts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
